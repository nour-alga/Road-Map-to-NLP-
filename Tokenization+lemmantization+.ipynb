{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP1zy7sdu6S8",
        "outputId": "5b919b7c-3f37-421d-9ad9-d2c4a0a296b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus =\"Bonjour , bienvenu a ce git ou tu vas apprendre les étapes de NLP pour pouvoir les utilisé dans les projets de Deep learning . Merci de me contacter via mail . j'écris alléatoirement . OK bon courage .\""
      ],
      "metadata": {
        "id": "JOZvVEBFwaNs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8bgwraKxB0w",
        "outputId": "e2a1be51-b237-4d67-fb30-694f56829704"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize # c'est pour passez d'un paragraph à une liste de phrase sent_tokenize (sent c'est pour sentence )\n",
        "sentences = sent_tokenize(corpus)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naff_cyWvxDT",
        "outputId": "2a53bba8-0a24-43a5-832e-303c098a9d5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bonjour , bienvenu a ce git ou tu vas apprendre les étapes de NLP pour pouvoir les utilisé dans les projets de Deep learning .', 'Merci de me contacter via mail .', \"j'écris alléatoirement .\", 'OK bon courage .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization pour passez de paragrah à des mots uniques ou de phrases à des mots uniques :\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(corpus)\n",
        "print(words)\n",
        "#ici la ponctuation est aussi considérer comme un mot on verra par la suite comment s'en débarassez et aussi comment s'en débarasser des mots qui nous ne donne pas beaucoup d'information par exemple je il ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oHr9VQDx3V4",
        "outputId": "73123e07-bbf2-44f5-bdab-93c435f1bc9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bonjour', ',', 'bienvenu', 'a', 'ce', 'git', 'ou', 'tu', 'vas', 'apprendre', 'les', 'étapes', 'de', 'NLP', 'pour', 'pouvoir', 'les', 'utilisé', 'dans', 'les', 'projets', 'de', 'Deep', 'learning', '.', 'Merci', 'de', 'me', 'contacter', 'via', 'mail', '.', \"j'écris\", 'alléatoirement', '.', 'OK', 'bon', 'courage', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# avant le mot j'écris à été considerer comme étant tout un mot si on veut le diviser il suffit d'utiliser wordpunct_tokenize :\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "words = wordpunct_tokenize(corpus)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgVVGCL_y1e1",
        "outputId": "fce0f417-4d60-42e7-9b87-d0120b3d3d6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bonjour', ',', 'bienvenu', 'a', 'ce', 'git', 'ou', 'tu', 'vas', 'apprendre', 'les', 'étapes', 'de', 'NLP', 'pour', 'pouvoir', 'les', 'utilisé', 'dans', 'les', 'projets', 'de', 'Deep', 'learning', '.', 'Merci', 'de', 'me', 'contacter', 'via', 'mail', '.', 'j', \"'\", 'écris', 'alléatoirement', '.', 'OK', 'bon', 'courage', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokenization*** is the first step in NLP our final goal is to transform our text to a numerical meaninful vectore to train our ML / DL model , tokenization help us transform our text into tokens corups=> sentences =>unique words ...Now our goal is to keep the important words that have meaning and simplifie it  our second Step which is ***Steamming*** will reduces the words into their steam word (reading=> read)"
      ],
      "metadata": {
        "id": "uRrBpGRh2lNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word + \" -----> \" + ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAkf60v02j2v",
        "outputId": "ad5b816f-4962-4a24-a5fa-4fbfa6a0b60e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour -----> bonjour\n",
            ", -----> ,\n",
            "bienvenu -----> bienvenu\n",
            "a -----> a\n",
            "ce -----> ce\n",
            "git -----> git\n",
            "ou -----> ou\n",
            "tu -----> tu\n",
            "vas -----> va\n",
            "apprendre -----> apprendr\n",
            "les -----> le\n",
            "étapes -----> étape\n",
            "de -----> de\n",
            "NLP -----> nlp\n",
            "pour -----> pour\n",
            "pouvoir -----> pouvoir\n",
            "les -----> le\n",
            "utilisé -----> utilisé\n",
            "dans -----> dan\n",
            "les -----> le\n",
            "projets -----> projet\n",
            "de -----> de\n",
            "Deep -----> deep\n",
            "learning -----> learn\n",
            ". -----> .\n",
            "Merci -----> merci\n",
            "de -----> de\n",
            "me -----> me\n",
            "contacter -----> contact\n",
            "via -----> via\n",
            "mail -----> mail\n",
            ". -----> .\n",
            "j -----> j\n",
            "' -----> '\n",
            "écris -----> écri\n",
            "alléatoirement -----> alléatoir\n",
            ". -----> .\n",
            "OK -----> ok\n",
            "bon -----> bon\n",
            "courage -----> courag\n",
            ". -----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# there is a problem okey with this steamming i can show it more\n",
        "print(ps.stem(\"congratulations\"))\n",
        "print(ps.stem(\"history\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrX7ZcHr8glB",
        "outputId": "10af49bf-a72a-481a-fd5b-273550941853"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "congratul\n",
            "histori\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "rs = RegexpStemmer('ing$|s$|e$|able$', min=0) #il supprime ing / s / able à la fin de chaque mots if we remove the dollar sign it will remove ing everywhere in the word\n",
        "print(rs.stem(\"reading\"))\n",
        "print(rs.stem(\"capable\"))\n",
        "print(rs.stem(\"congratulations\"))\n",
        "print(rs.stem(\"history\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d57SQnr89DW",
        "outputId": "ae0bfa54-51fe-4e21-dd48-76b4cabb30d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read\n",
            "cap\n",
            "congratulation\n",
            "history\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#snowball steammer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ss = SnowballStemmer(language='english')\n",
        "print(ss.stem(\"fairly\"))\n",
        "print(ps.stem(\"fairly\"))\n",
        "#this shows that snowball works where porteurstem fails\n",
        "#but stem still fail and change some words\n",
        "print(ss.stem(\"goes\"))\n",
        "#projects like chatbots we don't use steamming instead we use lemmantazation it fixes steaming problems"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQpTiQZl-3Cv",
        "outputId": "311734c2-3d6c-4bcb-e7ff-abbcba9382fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fair\n",
            "fairli\n",
            "goe\n"
          ]
        }
      ]
    }
  ]
}